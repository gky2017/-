{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "faa7358b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 신경망이 학습할 수 있도록 도와주는 지표 : 손실 함수\n",
    "# 학습의 목표는 이 손실 함수의 결괏값을 가장 작게 만드는 가중치 매개변수를 찾는 것\n",
    "\n",
    "# 이미지 분석 : 이미지에서 특징을 추출하고 그 특징의 패턴을 기계학습 기술로 학습하는 방법 : 컴퓨터 비전\n",
    "# 신경망은 이미지를 '있는 그대로' 학습함. 사람의 생각이 개입되지 않음.\n",
    "\n",
    "# 오버피팅(과적합) : 한 데이터셋에만 지나치게 최적화된 상태 이 과적합을 피하는 것이 머신러닝의 주요한 과제임.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2111c9d4",
   "metadata": {},
   "source": [
    "### 오차제곱합\n",
    "$$ E = \\frac1 2  \\sum_k (y_k = t_k)^2 $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "25ee0f8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.09750000000000003"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 오차제곱합은 가장 많이 쓰이는 손실 함수이다.\n",
    "import numpy as np\n",
    "\n",
    "def sse(y, t):\n",
    "    return 0.5 * np.sum((y-t) ** 2) \n",
    "\n",
    "t = [0, 0, 1, 0, 0, 0, 0, 0, 0, 0]\n",
    "y = [0.1, 0.05, 0.6, 0.0, 0.05, 0.1, 0.0, 0.1, 0.0, 0.0]\n",
    "sse(np.array(y), np.array(t))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b29ed3b0",
   "metadata": {},
   "source": [
    "## 교차 엔트로피 오차\n",
    "\n",
    "$$ E = -\\sum_k t_k  \\log y_k $$\n",
    "* $ y_k $는 신경망의 출력, $ t_k $ 는 정답 레이블"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcb1c982",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "307a7dad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.510825457099338"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 로그함수에서도 보이다싶이 x가 1일 때 y는 0이 되고, x가 0에 가까워질수록 y의 값은 점점 작아짐.\n",
    "# 식도 마찬가지로 정답에 해당하는 출력이 커질수록 0에 다가가다가 그 출력이 1일 때 0이 됨.\n",
    "import numpy as np\n",
    "\n",
    "def cee(y, t):\n",
    "    delta = 1e-7\n",
    "    return -np.sum(t * np.log(y + delta))  # 아주 작은 값인 delta를 더하는 이유는 np.log()가 0이 되지 않기 위함.\n",
    "\n",
    "t = [0, 0, 1, 0, 0, 0, 0, 0, 0, 0]\n",
    "y = [0.1, 0.05, 0.6, 0.0, 0.05, 0.1, 0.0, 0.1, 0.0, 0.0]\n",
    "cee(np.array(y), np.array(t))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd9350d1",
   "metadata": {},
   "source": [
    "## 모든 데이터에 대한 교차 엔트로피 오차\n",
    "\n",
    "$$ E = -\\frac1 N \\sum_n \\sum_k t_nk \\log y_nk  $$\n",
    "\n",
    "* 마지막에 N으로 나누어 정규화함. 이는 '평균 손실 함수'를 구하는 것임.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "cd0cb3af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading train-images-idx3-ubyte.gz ... \n",
      "Done\n",
      "Downloading train-labels-idx1-ubyte.gz ... \n",
      "Done\n",
      "Downloading t10k-images-idx3-ubyte.gz ... \n",
      "Done\n",
      "Downloading t10k-labels-idx1-ubyte.gz ... \n",
      "Done\n",
      "Converting train-images-idx3-ubyte.gz to NumPy Array ...\n",
      "Done\n",
      "Converting train-labels-idx1-ubyte.gz to NumPy Array ...\n",
      "Done\n",
      "Converting t10k-images-idx3-ubyte.gz to NumPy Array ...\n",
      "Done\n",
      "Converting t10k-labels-idx1-ubyte.gz to NumPy Array ...\n",
      "Done\n",
      "Creating pickle file ...\n",
      "Done!\n",
      "(60000, 784)\n",
      "(60000, 10)\n"
     ]
    }
   ],
   "source": [
    "import sys, os\n",
    "sys.path.append(\"/Users/yimjaekyoon/Library/CloudStorage/OneDrive-개인/ScratchDL\")\n",
    "import numpy as np\n",
    "from data.mnist import load_mnist\n",
    "\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(normalize = True, one_hot_label=True)\n",
    "\n",
    "print(x_train.shape)\n",
    "print(t_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "60758b77",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
       "       [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
       "       [0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 이 훈련 데이터에서 무작위로 10장만 빼내려면?\n",
    "train_size = x_train.shape[0]\n",
    "batch_size = 10\n",
    "batch_mask = np.random.choice(train_size, batch_size) # 0이상 train_size 미만의 수 중에서 무작위로 batch_size 개를 골라냄\n",
    "x_batch = x_train[batch_mask]\n",
    "t_batch = t_train[batch_mask]\n",
    "\n",
    "t_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "014a667e",
   "metadata": {},
   "source": [
    "### 배치용 교차 엔트로치 오차 구현하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "76e60e97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 미니배치 같은 배치 데이터를 지원하는 교차 엔트로피 오차(cce)는 ?\n",
    "batch_size = 10\n",
    "\n",
    "def cee(y, t):\n",
    "    if y.ndim == 1:\n",
    "        t = t.reshape(1, t.size)\n",
    "        y = y.reshape(1, y.size)    \n",
    "    batch_size = y.shape[0]\n",
    "    \n",
    "    return -np.sum(t * np.log(y + 1e-7)) / batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "423b9df6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 손실함수를 사용해야 하는 이유 : 신경망 학습에서는 최적의 매개변수를 탐색할 때 손실 함수의 값을 가능한 한 작게 하는 매개 변수 값을 찾음.\n",
    "# 이 때 매개변수의 미분(기울기)를 계산하고, 그 미분 값을 단서로 매개변수의 값을 서서싷 갱신하는 과정을 반복함.\n",
    "\n",
    "# 정확도를 지표로 삼아서는 안 되는 이유는 미분 값이 대부분 장소에서 0이 되어 매개변수를 갱신할 수 없기 때문.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c308a9a",
   "metadata": {},
   "source": [
    "## 수치 미분\n",
    "\n",
    "### 미분"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b3d4a977",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 미분 구현 - 나쁜 예\n",
    "def numerical_diff(f, x):\n",
    "    h = 10e-50\n",
    "    return (f(x + h) - f(x)) / h\n",
    "\n",
    "# 개선점 1\n",
    "# h는 매우 작은 값으로 대입하고 싶지만 이 방식은 반올림 오차 문제를 일으킴. : 너무 작은 값을 이용하면 컴퓨터로 계산하는데 문제가 된다.\n",
    "\n",
    "# 개선점 2\n",
    "# f의 차분에 관한 것 : (x + h)와 x 사이의 함수 f의 차분을 계산하고 있지만 이 계산에는 오차가 있다.\n",
    "# 진정한 미분은 x 위치의 함수의 기울기(접선)에 해당하지만 위 함수에서의 미분은 (x + h)와 x 사이의 기울기에 해당함.(전방 차분)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5bf15ae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 개선점을 반영 - 오차를 줄이기 위해 (x + h)와 (x - h)일 때의 함수 f의 차분을 구함(중심 차분)\n",
    "def numerical_diff(f, x):\n",
    "    h = 1e-4\n",
    "    return (f(x + h) - f(x - h)) / (2 * h)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ab02bd1",
   "metadata": {},
   "source": [
    "### 수치 미분의 예\n",
    "\n",
    "$ y = 0.01 x^2 + 0.1 x  $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b485bd9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def function_1(x):\n",
    "    return 0.01 * x ** 2 + 0.1 * x\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "x = np.arange(0.0, 20.0, 0.1)  # 0에서 20까지 0.1 간격의 배열 x를 만듬\n",
    "y = function_1(x)\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.plot(x, y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a60d6f1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1999999999990898\n",
      "0.2999999999986347\n"
     ]
    }
   ],
   "source": [
    "# x = 5, 10일때 값 미분해보자\n",
    "print(numerical_diff(function_1, 5))\n",
    "print(numerical_diff(function_1, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2148baaf",
   "metadata": {},
   "source": [
    "### 편미분\n",
    "\n",
    "$ f(x_0, x_1) = x^2_0 + x^2_1 $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f4103805",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.00000000000378\n",
      "7.999999999999119\n"
     ]
    }
   ],
   "source": [
    "def function_2(x):\n",
    "    return x[0] ** 2 + x[1] ** 2\n",
    "\n",
    "# 어느 변수에 대한 미분이냐를 구별해야 함.\n",
    "# 이와 같이 변수가 여럿인 함수에 대한 미분을 편미분이라고 함.\n",
    "\n",
    "def function_tmp1(x0):\n",
    "    return x0 ** 2 + 4 ** 2\n",
    "print(numerical_diff(function_tmp1, 3.0))\n",
    "\n",
    "def function_tmp2(x1):\n",
    "    return 3 ** 2 + x1 ** 2\n",
    "print(numerical_diff(function_tmp2, 4.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24029cfc",
   "metadata": {},
   "source": [
    "## 기울기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "54a63ffa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6. 8.]\n",
      "[0. 4.]\n",
      "[6. 0.]\n"
     ]
    }
   ],
   "source": [
    "# x0과 x1의 편미분을 동시에 계산하고 싶다면?\n",
    "# 모든 변수의 편미분을 벡터로 정리한 것을 기울기(gradient)라고 한다.\n",
    "def numerical_gradient(f, x):\n",
    "    h = 1e-4\n",
    "    grad = np.zeros_like(x)  # x와 형상이 같은 모든 원소가 0인 배열을 생성\n",
    "    \n",
    "    for idx in range(x.size):\n",
    "        tmp_val = x[idx]\n",
    "        # f(x + h) 계산\n",
    "        x[idx] = tmp_val + h\n",
    "        fxh1 = f(x)\n",
    "        \n",
    "        # f(x - h) 계산\n",
    "        x[idx] = tmp_val - h\n",
    "        fxh2 = f(x)\n",
    "        \n",
    "        grad[idx] = (fxh1 - fxh2) / (2 * h)\n",
    "        x[idx] = tmp_val # 값 복원\n",
    "        \n",
    "    return grad\n",
    "\n",
    "print(numerical_gradient(function_2, np.array([3.0, 4.0])))\n",
    "print(numerical_gradient(function_2, np.array([0.0, 2.0])))\n",
    "print(numerical_gradient(function_2, np.array([3.0, 0.0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9eb12b78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 기울기가 가리키는 쪽은 각 장소에서 함수의 출력 값을 가장 크게 줄이는 방향임."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "674899bc",
   "metadata": {},
   "source": [
    "### 경사 하강법\n",
    "\n",
    "* 일반적인 머신러닝 문제에서 손실 함수는 매우 복잡하다.\n",
    "* 매개변수 공간이 광대하여 어디가 최솟값이 되는 곳인지를 짐작할 수 없다.\n",
    "* 이런 상황에서 기울기를 잘 이용해 함수의 최솟값을 찾으려는 것이 경사 하강법이다.\n",
    "* 각 지점에서 함수의 값을 낮추는 방안을 제시하는 지표가 기울기 이지만 기울기가 가리키는 곳에 정말 함수의 최솟값이 있는지 보장할 수는 없다.\n",
    "* 경사 하강법은 현 위치에서 기울어진 방향으로 일정 거리만큼 이동하고, 또 이동한 곳에서 마찬가지로 기울기를 구하고 그 기울어진 방향으로 나아가기를 반복함.\n",
    "\n",
    "$$ x_0 = x_0 - \\eta \\frac {\\partial f} {\\partial x_0} $$\n",
    "$$ x_1 = x_1 - \\eta \\frac {\\partial f} {\\partial x_1} $$\n",
    "\n",
    "\n",
    "\n",
    "* $ \\eta $ 는 신경망 학습에서 학습률 이라고 한다. 이 학습률 값이 너무 크거나 작으면 '좋은 장소'를 찾아갈 수 없다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c4d6637b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-6.11110793e-10,  8.14814391e-10])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def grad_descent(f, init_x, lr = 0.01, step_num = 100):  \n",
    "    # f : 최적화하려는 함수, init_x : 초깃값 lr : 학습률, step_num : 반복 횟수\n",
    "    x = init_x\n",
    "    \n",
    "    for i in range(step_num):\n",
    "        grad = numerical_gradient(f, x)  # 함수의 기울기\n",
    "        x -= lr * grad\n",
    "    return x\n",
    "\n",
    "\n",
    "def function_2(x):\n",
    "    return x[0] ** 2 + x[1] ** 2\n",
    "\n",
    "init_x = np.array([-3.0, 4.0])\n",
    "grad_descent(function_2, init_x = init_x, lr = 0.1, step_num=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a5fd430e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-2.58983747e+13 -1.29524862e+12]\n",
      "[-2.99999994  3.99999992]\n"
     ]
    }
   ],
   "source": [
    "# 학습률이 너무 크면?\n",
    "init_x = np.array([-3.0, 4.0])\n",
    "print(grad_descent(function_2, init_x = init_x, lr = 10, step_num=100))\n",
    "\n",
    "# 학습률이 너무 작으면?\n",
    "init_x = np.array([-3.0, 4.0])\n",
    "print(grad_descent(function_2, init_x = init_x, lr = 1e-10, step_num=100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bf49fa7",
   "metadata": {},
   "source": [
    "### 신경망에서의 기울기\n",
    "\n",
    "* 여기서 말하는 기울기는 가중치 매개변수에 대한 손실 함수의 기울기이다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b94347cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-2.24267981  0.4063238  -1.87266483]\n",
      " [-0.56307679  0.78803347  0.05478087]]\n"
     ]
    }
   ],
   "source": [
    "import sys, os\n",
    "sys.path.append(\"/Users/yimjaekyoon/Library/CloudStorage/OneDrive-개인/ScratchDL\")\n",
    "import numpy as np\n",
    "from common.functions import softmax, cross_entropy_error\n",
    "from common.gradient import numerical_gradient\n",
    "\n",
    "class simpleNet:\n",
    "    def __init__(self):\n",
    "        self.W = np.random.randn(2, 3)  # 정규분포로 초기화\n",
    "        \n",
    "    def predict(self, x):\n",
    "        return np.dot(x, self.W)\n",
    "    \n",
    "    def loss(self, x, t):\n",
    "        z = self.predict(x)\n",
    "        y = softmax(z)\n",
    "        loss = cross_entropy_error(y, t)\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "net = simpleNet()\n",
    "print(net.W) # 가중치 매개변수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e407f9e9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.12311563  0.36006762  0.49144329]\n",
      "2\n",
      "0.8828446797292666\n"
     ]
    }
   ],
   "source": [
    "x = np.array([0.6, 0.9])\n",
    "p = net.predict(x)\n",
    "print(p)\n",
    "print(np.argmax(p))  # 최댓값의 index\n",
    "\n",
    "t = np.array([0,0,1])\n",
    "print(net.loss(x, t))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "50e8f28e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.13422615  0.21761105 -0.3518372 ]\n",
      " [ 0.20133922  0.32641658 -0.5277558 ]]\n"
     ]
    }
   ],
   "source": [
    "# 기울기를 구해보자\n",
    "# def f(W):\n",
    "#     return net.loss(x, t)\n",
    "\n",
    "# lambda 문법을 써보자\n",
    "f = lambda w : net.loss(x,t)\n",
    "dW = numerical_gradient(f, net.W)\n",
    "print(dW)\n",
    "\n",
    "# w11을 h만큼 늘리면 손실 함수의 값은 0.2h만큼 증가함.\n",
    "# w23을 h만큼 늘리면 손실 함수의 값은 0.5h만큼 감소함.\n",
    "\n",
    "# -> 손실함수를 줄인다는 관점에서는 w23은 양의 방향으로 갱신하고 w11은 음의 방향으로 갱신해야 함.\n",
    "# -> 한 번에 갱신되는 양에는 w23이 w11보다 크게 기여하고 있음."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b749520",
   "metadata": {},
   "source": [
    "### 2층 신경망 클래스 구현하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "1bea8932",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.append(\"/Users/yimjaekyoon/Library/CloudStorage/OneDrive-개인/ScratchDL\")\n",
    "import numpy as np\n",
    "from common.functions import *\n",
    "from common.gradient import numerical_gradient\n",
    "\n",
    "class TwoLayerNet:\n",
    "    # 초기화 수행\n",
    "    def __init__(self, input_size, hidden_size, output_size, weight_init_std =  0.01):\n",
    "        # 가중치 초기화\n",
    "        self.params = {}\n",
    "        self.params['W1'] = weight_init_std * np.random.randn(input_size, hidden_size)\n",
    "        self.params['b1'] = np.zeros(hidden_size)\n",
    "        self.params['W2'] = weight_init_std * np.random.randn(hidden_size, output_size)\n",
    "        self.params['b2'] = np.zeros(output_size)\n",
    "    \n",
    "    # 예측을 수행\n",
    "    def predict(self, x):\n",
    "        W1, W2 = self.params['W1'], self.params['W2']\n",
    "        b1, b2 = self.params['b1'], self.params['b2']\n",
    "        \n",
    "        a1 = np.dot(x, W1) + b1\n",
    "        z1 = sigmoid(a1)\n",
    "        a2 = np.dot(z1, W2) + b2\n",
    "        y = softmax(a2)\n",
    "        \n",
    "        return y\n",
    "    \n",
    "    # 손실 함수의 값을 구함\n",
    "    def loss(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        \n",
    "        return cross_entropy_error(y, t)\n",
    "    \n",
    "    # 정확도를 구함\n",
    "    def accuracy(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        y = np.argmax(y, axis = 1)\n",
    "        t = np.argmax(t, axis = 1)\n",
    "        \n",
    "        accuracy = np.sum(y == t) / float(x.shape[0])\n",
    "        return accuracy\n",
    "    \n",
    "    # 가중치 매개변수의 기울기를 구함\n",
    "    def numerical_gradient(self, x, t):\n",
    "        loss_W = lambda W: self.loss(x, t)\n",
    "\n",
    "        grads = {}\n",
    "        grads['W1'] = numerical_gradient(loss_W, self.params['W1'])\n",
    "        grads['b1'] = numerical_gradient(loss_W, self.params['b1'])\n",
    "        grads['W2'] = numerical_gradient(loss_W, self.params['W2'])\n",
    "        grads['b2'] = numerical_gradient(loss_W, self.params['b2'])\n",
    "\n",
    "        return grads\n",
    "    \n",
    "    # n_g의 성능 개선판\n",
    "    def gradient(self, x, t):\n",
    "        W1, W2 = self.params['W1'], self.params['W2']\n",
    "        b1, b2 = self.params['b1'], self.params['b2']\n",
    "        grads = {}\n",
    "\n",
    "        batch_num = x.shape[0]\n",
    "\n",
    "        # forward\n",
    "        a1 = np.dot(x, W1) + b1\n",
    "        z1 = sigmoid(a1)\n",
    "        a2 = np.dot(z1, W2) + b2\n",
    "        y = softmax(a2)\n",
    "\n",
    "        # backward\n",
    "        dy = (y - t) / batch_num  # ????\n",
    "        grads['W2'] = np.dot(z1.T, dy)\n",
    "        grads['b2'] = np.sum(dy, axis=0)\n",
    "\n",
    "        da1 = np.dot(dy, W2.T)\n",
    "        dz1 = sigmoid_grad(a1) * da1\n",
    "        grads['W1'] = np.dot(x.T, dz1)\n",
    "        grads['b1'] = np.sum(dz1, axis=0)\n",
    "\n",
    "        return grads\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a00bc5e",
   "metadata": {},
   "source": [
    "### 미니배치 학습 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "6ca2dc8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from data.mnist import load_mnist\n",
    "from two_layer_net import TwoLayerNet\n",
    "\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)\n",
    "train_loss_list = []\n",
    "\n",
    "# 하이퍼파라미터\n",
    "iters_num = 10000 # 반복 횟수\n",
    "train_size = x_train.shape[0]\n",
    "batch_size = 100 # 미니배치 크기\n",
    "learning_rate = 0.1\n",
    "\n",
    "network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)\n",
    "\n",
    "for i in range(iters_num):\n",
    "    # 미니배치 획득\n",
    "    batch_mask = np.random.choice(train_size, batch_size)\n",
    "    x_batch = x_train[batch_mask]\n",
    "    t_batch = t_train[batch_mask]\n",
    "    \n",
    "    # 기울기 계산\n",
    "    grad = network.gradient(x_batch, t_batch)\n",
    "    \n",
    "    # 매개변수 갱신\n",
    "    for key in ('W1', 'b1', 'W2', 'b2'):\n",
    "        network.params[key] -= learning_rate * grad[key]\n",
    "    \n",
    "    # 학습 경과 기록\n",
    "    loss = network.loss(x_batch, t_batch)\n",
    "    train_loss_list.append(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "fbcd3939",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 그래프로 나타내보기\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "x = np.arange(0.0, 10000.0, 1.0)\n",
    "y = train_loss_list\n",
    "plt.plot(x,y)\n",
    "plt.ylim(0, 3.0) # y축의 범위\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "676b7228",
   "metadata": {},
   "source": [
    "### 시험 데이터로 평가하기\n",
    "\n",
    "* 신경망 학습에서는 훈련 데이터 외의 데이터를 올바르게 인식하는지를 확인해야 한다.\n",
    "* 다른 말로 오버피팅을 일으키지 않는지 확인해야 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "3cec8702",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train acc, test acc | 0.09736666666666667, 0.0982\n",
      "train acc, test acc | 0.7917666666666666, 0.7967\n",
      "train acc, test acc | 0.8775, 0.882\n",
      "train acc, test acc | 0.8967, 0.9011\n",
      "train acc, test acc | 0.9075333333333333, 0.9096\n",
      "train acc, test acc | 0.91345, 0.9157\n",
      "train acc, test acc | 0.9183, 0.921\n",
      "train acc, test acc | 0.9225666666666666, 0.9258\n",
      "train acc, test acc | 0.9259, 0.9267\n",
      "train acc, test acc | 0.92985, 0.932\n",
      "train acc, test acc | 0.93285, 0.9339\n",
      "train acc, test acc | 0.9352833333333334, 0.935\n",
      "train acc, test acc | 0.9379, 0.9384\n",
      "train acc, test acc | 0.9403333333333334, 0.9415\n",
      "train acc, test acc | 0.9423666666666667, 0.943\n",
      "train acc, test acc | 0.9439166666666666, 0.9445\n",
      "train acc, test acc | 0.9467833333333333, 0.946\n",
      "train acc, test acc | 0.9484, 0.9473\n",
      "train acc, test acc | 0.9491166666666667, 0.9493\n",
      "train acc, test acc | 0.9515, 0.9497\n",
      "train acc, test acc | 0.9520333333333333, 0.9511\n",
      "train acc, test acc | 0.9535, 0.9516\n",
      "train acc, test acc | 0.9548, 0.9517\n",
      "train acc, test acc | 0.9562666666666667, 0.9532\n",
      "train acc, test acc | 0.957, 0.955\n",
      "train acc, test acc | 0.9581333333333333, 0.9556\n",
      "train acc, test acc | 0.9590333333333333, 0.9555\n",
      "train acc, test acc | 0.9601, 0.9564\n",
      "train acc, test acc | 0.9608833333333333, 0.957\n",
      "train acc, test acc | 0.9619333333333333, 0.9574\n",
      "train acc, test acc | 0.9627666666666667, 0.9591\n",
      "train acc, test acc | 0.9638333333333333, 0.9586\n",
      "train acc, test acc | 0.9641666666666666, 0.9587\n",
      "train acc, test acc | 0.9648833333333333, 0.9594\n",
      "train acc, test acc | 0.9658333333333333, 0.96\n",
      "train acc, test acc | 0.9663, 0.9611\n",
      "train acc, test acc | 0.967, 0.9599\n",
      "train acc, test acc | 0.9676, 0.9607\n",
      "train acc, test acc | 0.9679833333333333, 0.9611\n",
      "train acc, test acc | 0.9688666666666667, 0.9616\n",
      "train acc, test acc | 0.9693833333333334, 0.963\n",
      "train acc, test acc | 0.96995, 0.963\n",
      "train acc, test acc | 0.97055, 0.9626\n",
      "train acc, test acc | 0.9705, 0.9622\n",
      "train acc, test acc | 0.9716166666666667, 0.964\n",
      "train acc, test acc | 0.9724166666666667, 0.9642\n",
      "train acc, test acc | 0.97225, 0.9638\n",
      "train acc, test acc | 0.9731166666666666, 0.9652\n",
      "train acc, test acc | 0.9734333333333334, 0.9653\n",
      "train acc, test acc | 0.9733666666666667, 0.965\n",
      "train acc, test acc | 0.9739333333333333, 0.9647\n",
      "train acc, test acc | 0.9746666666666667, 0.9658\n",
      "train acc, test acc | 0.9745666666666667, 0.9658\n",
      "train acc, test acc | 0.9748666666666667, 0.9652\n",
      "train acc, test acc | 0.9750166666666666, 0.9661\n",
      "train acc, test acc | 0.9756, 0.9658\n",
      "train acc, test acc | 0.97635, 0.9666\n",
      "train acc, test acc | 0.9765333333333334, 0.9662\n",
      "train acc, test acc | 0.97705, 0.9672\n",
      "train acc, test acc | 0.9772666666666666, 0.9672\n",
      "train acc, test acc | 0.9774666666666667, 0.9674\n",
      "train acc, test acc | 0.9779166666666667, 0.9669\n",
      "train acc, test acc | 0.9779333333333333, 0.9679\n",
      "train acc, test acc | 0.9786166666666667, 0.9675\n",
      "train acc, test acc | 0.9784833333333334, 0.9685\n",
      "train acc, test acc | 0.9789, 0.9678\n",
      "train acc, test acc | 0.9792, 0.9684\n",
      "train acc, test acc | 0.97975, 0.9686\n",
      "train acc, test acc | 0.9793333333333333, 0.9682\n",
      "train acc, test acc | 0.9797833333333333, 0.9686\n",
      "train acc, test acc | 0.9803333333333333, 0.969\n",
      "train acc, test acc | 0.9808, 0.9696\n",
      "train acc, test acc | 0.9802166666666666, 0.9683\n",
      "train acc, test acc | 0.9807666666666667, 0.9696\n",
      "train acc, test acc | 0.9814, 0.9697\n",
      "train acc, test acc | 0.9813166666666666, 0.9706\n",
      "train acc, test acc | 0.98165, 0.9702\n",
      "train acc, test acc | 0.9820166666666666, 0.9699\n",
      "train acc, test acc | 0.9818166666666667, 0.9701\n",
      "train acc, test acc | 0.9824333333333334, 0.9714\n",
      "train acc, test acc | 0.9822333333333333, 0.97\n",
      "train acc, test acc | 0.9827166666666667, 0.9704\n",
      "train acc, test acc | 0.9828833333333333, 0.971\n",
      "train acc, test acc | 0.98295, 0.9711\n",
      "train acc, test acc | 0.9830666666666666, 0.9708\n",
      "train acc, test acc | 0.9834333333333334, 0.9708\n",
      "train acc, test acc | 0.9832833333333333, 0.9706\n",
      "train acc, test acc | 0.98385, 0.9708\n",
      "train acc, test acc | 0.9838166666666667, 0.9713\n",
      "train acc, test acc | 0.9841166666666666, 0.9713\n",
      "train acc, test acc | 0.98455, 0.9708\n",
      "train acc, test acc | 0.9843333333333333, 0.9709\n",
      "train acc, test acc | 0.98465, 0.9708\n",
      "train acc, test acc | 0.9847666666666667, 0.9716\n",
      "train acc, test acc | 0.9849333333333333, 0.9708\n",
      "train acc, test acc | 0.9851, 0.9715\n",
      "train acc, test acc | 0.9854333333333334, 0.9718\n",
      "train acc, test acc | 0.9852833333333333, 0.9724\n",
      "train acc, test acc | 0.9855, 0.972\n",
      "train acc, test acc | 0.9856333333333334, 0.9724\n",
      "train acc, test acc | 0.9855666666666667, 0.9712\n",
      "train acc, test acc | 0.9861666666666666, 0.9715\n",
      "train acc, test acc | 0.9860666666666666, 0.9719\n",
      "train acc, test acc | 0.9863, 0.9724\n",
      "train acc, test acc | 0.9867, 0.973\n",
      "train acc, test acc | 0.9867333333333334, 0.9735\n",
      "train acc, test acc | 0.9871, 0.9723\n",
      "train acc, test acc | 0.98725, 0.9729\n",
      "train acc, test acc | 0.9872, 0.9729\n",
      "train acc, test acc | 0.9872333333333333, 0.9722\n",
      "train acc, test acc | 0.9872333333333333, 0.9729\n",
      "train acc, test acc | 0.9874833333333334, 0.9722\n",
      "train acc, test acc | 0.98765, 0.9731\n",
      "train acc, test acc | 0.9879333333333333, 0.9735\n",
      "train acc, test acc | 0.9878166666666667, 0.9737\n",
      "train acc, test acc | 0.9878333333333333, 0.9738\n",
      "train acc, test acc | 0.98805, 0.9733\n",
      "train acc, test acc | 0.9879833333333333, 0.9738\n",
      "train acc, test acc | 0.9881166666666666, 0.9731\n",
      "train acc, test acc | 0.9882666666666666, 0.9743\n",
      "train acc, test acc | 0.98855, 0.974\n",
      "train acc, test acc | 0.9883, 0.9734\n",
      "train acc, test acc | 0.9883, 0.9734\n",
      "train acc, test acc | 0.9887166666666667, 0.9726\n",
      "train acc, test acc | 0.9887333333333334, 0.9734\n",
      "train acc, test acc | 0.98895, 0.9731\n",
      "train acc, test acc | 0.9889166666666667, 0.974\n",
      "train acc, test acc | 0.9893833333333333, 0.9735\n",
      "train acc, test acc | 0.98915, 0.9743\n",
      "train acc, test acc | 0.9897166666666667, 0.974\n",
      "train acc, test acc | 0.9896, 0.9737\n",
      "train acc, test acc | 0.9893333333333333, 0.9732\n",
      "train acc, test acc | 0.9898666666666667, 0.9733\n",
      "train acc, test acc | 0.98975, 0.9727\n",
      "train acc, test acc | 0.9899166666666667, 0.9735\n",
      "train acc, test acc | 0.9902, 0.9731\n",
      "train acc, test acc | 0.9901, 0.9737\n",
      "train acc, test acc | 0.99015, 0.9735\n",
      "train acc, test acc | 0.9901333333333333, 0.9733\n",
      "train acc, test acc | 0.9901666666666666, 0.9734\n",
      "train acc, test acc | 0.9903666666666666, 0.9733\n",
      "train acc, test acc | 0.9906, 0.9743\n",
      "train acc, test acc | 0.99075, 0.9743\n",
      "train acc, test acc | 0.9906166666666667, 0.973\n",
      "train acc, test acc | 0.9910166666666667, 0.9739\n",
      "train acc, test acc | 0.9909666666666667, 0.974\n",
      "train acc, test acc | 0.991, 0.9734\n",
      "train acc, test acc | 0.9912, 0.9736\n",
      "train acc, test acc | 0.9913166666666666, 0.9739\n",
      "train acc, test acc | 0.9914, 0.9742\n",
      "train acc, test acc | 0.9913166666666666, 0.9741\n",
      "train acc, test acc | 0.9912, 0.9742\n",
      "train acc, test acc | 0.9916666666666667, 0.9743\n",
      "train acc, test acc | 0.9915833333333334, 0.9738\n",
      "train acc, test acc | 0.9916333333333334, 0.974\n",
      "train acc, test acc | 0.9917166666666667, 0.974\n",
      "train acc, test acc | 0.992, 0.9742\n",
      "train acc, test acc | 0.99185, 0.974\n",
      "train acc, test acc | 0.9918833333333333, 0.974\n",
      "train acc, test acc | 0.9919166666666667, 0.9744\n",
      "train acc, test acc | 0.9919333333333333, 0.9736\n",
      "train acc, test acc | 0.9921333333333333, 0.9745\n",
      "train acc, test acc | 0.9922666666666666, 0.9743\n",
      "train acc, test acc | 0.9924166666666666, 0.975\n",
      "train acc, test acc | 0.9924333333333333, 0.9746\n",
      "train acc, test acc | 0.9925166666666667, 0.9748\n",
      "train acc, test acc | 0.9926166666666667, 0.9739\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from data.mnist import load_mnist\n",
    "from two_layer_net import TwoLayerNet\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)\n",
    "\n",
    "network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)\n",
    "\n",
    "# 하이퍼파라미터\n",
    "iters_num = 100000\n",
    "train_size = x_train.shape[0]\n",
    "batch_size = 100\n",
    "learning_rate = 0.1\n",
    "\n",
    "train_loss_list = []\n",
    "train_acc_list = []\n",
    "test_acc_list = []\n",
    "\n",
    "iter_per_epoch = max(train_size / batch_size, 1)\n",
    "\n",
    "for i in range(iters_num):\n",
    "    # 미니배치 획득\n",
    "    batch_mask = np.random.choice(train_size, batch_size)\n",
    "    x_batch = x_train[batch_mask]\n",
    "    t_batch = t_train[batch_mask]\n",
    "    \n",
    "    # 기울기 계산\n",
    "    grad = network.gradient(x_batch, t_batch)\n",
    "    \n",
    "    # 매개변수 갱신\n",
    "    for key in ('W1', 'b1', 'W2', 'b2'):\n",
    "        network.params[key] -= learning_rate * grad[key]\n",
    "    \n",
    "    # 학습 경과 기록\n",
    "    loss = network.loss(x_batch, t_batch)\n",
    "    train_loss_list.append(loss)\n",
    "    \n",
    "    # 1 epoch 당 정확도 계산\n",
    "    if i % iter_per_epoch == 0:\n",
    "        train_acc = network.accuracy(x_train, t_train)\n",
    "        test_acc = network.accuracy(x_test, t_test)\n",
    "        train_acc_list.append(train_acc)\n",
    "        test_acc_list.append(test_acc)\n",
    "        print(\"train acc, test acc | \" + str(train_acc) + \", \" + str(test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bc09b44",
   "metadata": {},
   "outputs": [],
   "source": [
    "#"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
